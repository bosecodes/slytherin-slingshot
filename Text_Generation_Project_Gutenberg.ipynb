{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Text Generation - Project Gutenberg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bosecodes/slytherin-slingshot/blob/master/Text_Generation_Project_Gutenberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuW2dckBrax",
        "colab_type": "text"
      },
      "source": [
        "# Develop a Small LSTM Recurrent Neural Network\n",
        "\n",
        "In this section we will develop a simple LSTM network to learn the sequences of characters from Alice in Wonderland. In the next section, we will use this model to generate new sequences of characters.\n",
        "\n",
        "Let's start off by importing the classes and the functions we intend to use to train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojb653boBra0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2aaf0cb0-4f59-409a-ebb1-590519b69e99"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.utils import np_utils"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eva00JI3CFxI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Qmj4j9Bra-",
        "colab_type": "text"
      },
      "source": [
        "Next we need to load the ASCII text for the book into memory and convert all the characters to lowercase to reduce the vocabulary that the network must learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN16NQmyBrbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and convert to lowercase\n",
        "location = '/content/wonderland.txt'\n",
        "raw_text = open(location, 'r', encoding = 'utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XntG42aEBrbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c564803c-d20b-404e-8acf-b061b4e4a6ff"
      },
      "source": [
        "raw_text[:100]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"project gutenberg's alice's adventures in wonderland, by lewis carroll\\n\\nthis ebook is for the use of\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU7skywrBrbR",
        "colab_type": "text"
      },
      "source": [
        "Now that the book is loaded, we must prepare the data for modelling by the neural network. We can't model the characters directly, we instead must convert the characters to integers.\n",
        "\n",
        "We can do this easily by first creating a set of all the distinct characters in the book, then creating a map of each character to a unique integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMgD6NUgBrbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c,i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvzXwWI-Brba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02ca9089-c518-48d6-c653-98cec7ed7d47"
      },
      "source": [
        "char_to_int"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '#': 4,\n",
              " '$': 5,\n",
              " '%': 6,\n",
              " \"'\": 7,\n",
              " '(': 8,\n",
              " ')': 9,\n",
              " '*': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '1': 16,\n",
              " '2': 17,\n",
              " '3': 18,\n",
              " '4': 19,\n",
              " '5': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " ':': 25,\n",
              " ';': 26,\n",
              " '?': 27,\n",
              " '@': 28,\n",
              " '[': 29,\n",
              " ']': 30,\n",
              " '_': 31,\n",
              " 'a': 32,\n",
              " 'b': 33,\n",
              " 'c': 34,\n",
              " 'd': 35,\n",
              " 'e': 36,\n",
              " 'f': 37,\n",
              " 'g': 38,\n",
              " 'h': 39,\n",
              " 'i': 40,\n",
              " 'j': 41,\n",
              " 'k': 42,\n",
              " 'l': 43,\n",
              " 'm': 44,\n",
              " 'n': 45,\n",
              " 'o': 46,\n",
              " 'p': 47,\n",
              " 'q': 48,\n",
              " 'r': 49,\n",
              " 's': 50,\n",
              " 't': 51,\n",
              " 'u': 52,\n",
              " 'v': 53,\n",
              " 'w': 54,\n",
              " 'x': 55,\n",
              " 'y': 56,\n",
              " 'z': 57}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61-RuP9mBrbh",
        "colab_type": "text"
      },
      "source": [
        "We can further remove some other characters that would further clean up the dataset that will reduce the vocabulary and improve the modelling process.\n",
        "\n",
        "Now that the book has been loaded and the mapping prepared, we can summarise the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh4T5dWDBrbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26faeb32-e079-4cb6-a138-75f460c21be0"
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print('Total Characters: ', n_chars)\n",
        "print('Total Vocab: ', n_vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  163779\n",
            "Total Vocab:  58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMvqvrJBrbs",
        "colab_type": "text"
      },
      "source": [
        "Thus the book has about 160,000 characters and that when converted to lowercase, there are only 58 distinct characters in the vocabulary for the network to learn. Much more than the 26 in the alphabet.\n",
        "\n",
        "We need to now define the training data for our network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
        "\n",
        "In this tutorial we will split the book text up to subsequences of 100 characters, an arbitrary length. We could just as easily split the data by sentences and pad the shorter sequences and truncate the longer ones.\n",
        "\n",
        "Each training pattern is comprised of 100 time steps of one character (X) followed by one character output(y). When creating these sequences, we slide this window along the whole book, one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it.(except the first 100 characters ofcourse).\n",
        "\n",
        "For example, if the sequence length is 5 (for simplicity, then the two training patterns would be as follows:\n",
        "\n",
        "CHAPT -> E\n",
        "\n",
        "HAPTE -> R\n",
        "\n",
        "As we split up the book into sequences, we convert the characters into integers according to the lookup table(dictionary), that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKGnglCABrbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a0ee4c4-39e6-4129-b013-ab8ff3a4811b"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i: i + seq_length]\n",
        "    seq_out = raw_text[i+seq_length] # that is, the first character\n",
        "    # after the sequence is complete\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print('Total Patterns: ', n_patterns)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  163679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NB2QMBWBrb1",
        "colab_type": "text"
      },
      "source": [
        "Thus the number of total pattterns we have 100 less patterns, caused by barring the first 100 characters. We have one training pattern to predict each of the remaining characters.\n",
        "\n",
        "Now that we have the prepared training data, we need to transform it so that it is suitable for use with Keras.\n",
        "- First we must transform the input sequences to the form [samples, time steps, features] expected by the LSTM network.\n",
        "- Next we need to rescale the integers to the range of 0-1 to make the patterns easier to learn by the LSTM network that uses a sigmoid activation function by default\n",
        "- Finally we need to convert the output patterns(single characters converted to integers) to a one hot encoding. This is so that we can configure the network to predict the probability of the 57 different characters in teh vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted to a Sparse Vector with a length of 57, full of zeroes except with a 1 in the column where teh letter that the pattern represents.\n",
        "\n",
        "For example, when 'n' integer value 31 is one hot encoded, it looks like:\n",
        "\n",
        "[ 0 0 0 0 0 0 0 0 0 0\n",
        "  0 0 0 1 0 0 0 0 0 0\n",
        "  0 0 0 0 0 0 ]\n",
        "\n",
        "We can implement these steps as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMRTsX_fBrb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "08aefc8c-a845-4f3d-a115-77ebd2408ee0"
      },
      "source": [
        "print(np.reshape(dataX, (n_patterns, seq_length), ))\n",
        "# that is how the input data is supposed to be\n",
        "# however we add a third variable in the tuple called '1'\n",
        "# this is to wrap the entire 2-D array to a \n",
        "# 3-D array"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[47 49 46 ...  1 46 37]\n",
            " [49 46 41 ... 46 37  1]\n",
            " [46 41 36 ... 37  1 32]\n",
            " ...\n",
            " [ 1 51 46 ... 33 46 46]\n",
            " [51 46  1 ... 46 46 42]\n",
            " [46  1 39 ... 46 42 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS6xQnWwBrb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ce03b53-a3af-4795-a3d2-b514ce800143"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length,1), )\n",
        "# Normalize\n",
        "print(X[:5])\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# One Hot Encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[47]\n",
            "  [49]\n",
            "  [46]\n",
            "  [41]\n",
            "  [36]\n",
            "  [34]\n",
            "  [51]\n",
            "  [ 1]\n",
            "  [38]\n",
            "  [52]\n",
            "  [51]\n",
            "  [36]\n",
            "  [45]\n",
            "  [33]\n",
            "  [36]\n",
            "  [49]\n",
            "  [38]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [43]\n",
            "  [40]\n",
            "  [34]\n",
            "  [36]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [35]\n",
            "  [53]\n",
            "  [36]\n",
            "  [45]\n",
            "  [51]\n",
            "  [52]\n",
            "  [49]\n",
            "  [36]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [45]\n",
            "  [ 1]\n",
            "  [54]\n",
            "  [46]\n",
            "  [45]\n",
            "  [35]\n",
            "  [36]\n",
            "  [49]\n",
            "  [43]\n",
            "  [32]\n",
            "  [45]\n",
            "  [35]\n",
            "  [11]\n",
            "  [ 1]\n",
            "  [33]\n",
            "  [56]\n",
            "  [ 1]\n",
            "  [43]\n",
            "  [36]\n",
            "  [54]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [34]\n",
            "  [32]\n",
            "  [49]\n",
            "  [49]\n",
            "  [46]\n",
            "  [43]\n",
            "  [43]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [51]\n",
            "  [39]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [36]\n",
            "  [33]\n",
            "  [46]\n",
            "  [46]\n",
            "  [42]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [37]\n",
            "  [46]\n",
            "  [49]\n",
            "  [ 1]\n",
            "  [51]\n",
            "  [39]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [52]\n",
            "  [50]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [46]\n",
            "  [37]]\n",
            "\n",
            " [[49]\n",
            "  [46]\n",
            "  [41]\n",
            "  [36]\n",
            "  [34]\n",
            "  [51]\n",
            "  [ 1]\n",
            "  [38]\n",
            "  [52]\n",
            "  [51]\n",
            "  [36]\n",
            "  [45]\n",
            "  [33]\n",
            "  [36]\n",
            "  [49]\n",
            "  [38]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [43]\n",
            "  [40]\n",
            "  [34]\n",
            "  [36]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [35]\n",
            "  [53]\n",
            "  [36]\n",
            "  [45]\n",
            "  [51]\n",
            "  [52]\n",
            "  [49]\n",
            "  [36]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [45]\n",
            "  [ 1]\n",
            "  [54]\n",
            "  [46]\n",
            "  [45]\n",
            "  [35]\n",
            "  [36]\n",
            "  [49]\n",
            "  [43]\n",
            "  [32]\n",
            "  [45]\n",
            "  [35]\n",
            "  [11]\n",
            "  [ 1]\n",
            "  [33]\n",
            "  [56]\n",
            "  [ 1]\n",
            "  [43]\n",
            "  [36]\n",
            "  [54]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [34]\n",
            "  [32]\n",
            "  [49]\n",
            "  [49]\n",
            "  [46]\n",
            "  [43]\n",
            "  [43]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [51]\n",
            "  [39]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [36]\n",
            "  [33]\n",
            "  [46]\n",
            "  [46]\n",
            "  [42]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [37]\n",
            "  [46]\n",
            "  [49]\n",
            "  [ 1]\n",
            "  [51]\n",
            "  [39]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [52]\n",
            "  [50]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [46]\n",
            "  [37]\n",
            "  [ 1]]\n",
            "\n",
            " [[46]\n",
            "  [41]\n",
            "  [36]\n",
            "  [34]\n",
            "  [51]\n",
            "  [ 1]\n",
            "  [38]\n",
            "  [52]\n",
            "  [51]\n",
            "  [36]\n",
            "  [45]\n",
            "  [33]\n",
            "  [36]\n",
            "  [49]\n",
            "  [38]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [43]\n",
            "  [40]\n",
            "  [34]\n",
            "  [36]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [35]\n",
            "  [53]\n",
            "  [36]\n",
            "  [45]\n",
            "  [51]\n",
            "  [52]\n",
            "  [49]\n",
            "  [36]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [45]\n",
            "  [ 1]\n",
            "  [54]\n",
            "  [46]\n",
            "  [45]\n",
            "  [35]\n",
            "  [36]\n",
            "  [49]\n",
            "  [43]\n",
            "  [32]\n",
            "  [45]\n",
            "  [35]\n",
            "  [11]\n",
            "  [ 1]\n",
            "  [33]\n",
            "  [56]\n",
            "  [ 1]\n",
            "  [43]\n",
            "  [36]\n",
            "  [54]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [34]\n",
            "  [32]\n",
            "  [49]\n",
            "  [49]\n",
            "  [46]\n",
            "  [43]\n",
            "  [43]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [51]\n",
            "  [39]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [36]\n",
            "  [33]\n",
            "  [46]\n",
            "  [46]\n",
            "  [42]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [37]\n",
            "  [46]\n",
            "  [49]\n",
            "  [ 1]\n",
            "  [51]\n",
            "  [39]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [52]\n",
            "  [50]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [46]\n",
            "  [37]\n",
            "  [ 1]\n",
            "  [32]]\n",
            "\n",
            " [[41]\n",
            "  [36]\n",
            "  [34]\n",
            "  [51]\n",
            "  [ 1]\n",
            "  [38]\n",
            "  [52]\n",
            "  [51]\n",
            "  [36]\n",
            "  [45]\n",
            "  [33]\n",
            "  [36]\n",
            "  [49]\n",
            "  [38]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [43]\n",
            "  [40]\n",
            "  [34]\n",
            "  [36]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [35]\n",
            "  [53]\n",
            "  [36]\n",
            "  [45]\n",
            "  [51]\n",
            "  [52]\n",
            "  [49]\n",
            "  [36]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [45]\n",
            "  [ 1]\n",
            "  [54]\n",
            "  [46]\n",
            "  [45]\n",
            "  [35]\n",
            "  [36]\n",
            "  [49]\n",
            "  [43]\n",
            "  [32]\n",
            "  [45]\n",
            "  [35]\n",
            "  [11]\n",
            "  [ 1]\n",
            "  [33]\n",
            "  [56]\n",
            "  [ 1]\n",
            "  [43]\n",
            "  [36]\n",
            "  [54]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [34]\n",
            "  [32]\n",
            "  [49]\n",
            "  [49]\n",
            "  [46]\n",
            "  [43]\n",
            "  [43]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [51]\n",
            "  [39]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [36]\n",
            "  [33]\n",
            "  [46]\n",
            "  [46]\n",
            "  [42]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [37]\n",
            "  [46]\n",
            "  [49]\n",
            "  [ 1]\n",
            "  [51]\n",
            "  [39]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [52]\n",
            "  [50]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [46]\n",
            "  [37]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [45]]\n",
            "\n",
            " [[36]\n",
            "  [34]\n",
            "  [51]\n",
            "  [ 1]\n",
            "  [38]\n",
            "  [52]\n",
            "  [51]\n",
            "  [36]\n",
            "  [45]\n",
            "  [33]\n",
            "  [36]\n",
            "  [49]\n",
            "  [38]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [43]\n",
            "  [40]\n",
            "  [34]\n",
            "  [36]\n",
            "  [ 7]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [35]\n",
            "  [53]\n",
            "  [36]\n",
            "  [45]\n",
            "  [51]\n",
            "  [52]\n",
            "  [49]\n",
            "  [36]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [45]\n",
            "  [ 1]\n",
            "  [54]\n",
            "  [46]\n",
            "  [45]\n",
            "  [35]\n",
            "  [36]\n",
            "  [49]\n",
            "  [43]\n",
            "  [32]\n",
            "  [45]\n",
            "  [35]\n",
            "  [11]\n",
            "  [ 1]\n",
            "  [33]\n",
            "  [56]\n",
            "  [ 1]\n",
            "  [43]\n",
            "  [36]\n",
            "  [54]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [34]\n",
            "  [32]\n",
            "  [49]\n",
            "  [49]\n",
            "  [46]\n",
            "  [43]\n",
            "  [43]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [51]\n",
            "  [39]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [36]\n",
            "  [33]\n",
            "  [46]\n",
            "  [46]\n",
            "  [42]\n",
            "  [ 1]\n",
            "  [40]\n",
            "  [50]\n",
            "  [ 1]\n",
            "  [37]\n",
            "  [46]\n",
            "  [49]\n",
            "  [ 1]\n",
            "  [51]\n",
            "  [39]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [52]\n",
            "  [50]\n",
            "  [36]\n",
            "  [ 1]\n",
            "  [46]\n",
            "  [37]\n",
            "  [ 1]\n",
            "  [32]\n",
            "  [45]\n",
            "  [56]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo9kI-YhBrcF",
        "colab_type": "text"
      },
      "source": [
        "We can now define our LSTM model. Here, we define a single LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense (Fully connected) layer, using the softmax activation function to output the probability prediction for each of the 57 characters between 0 and 1.\n",
        "\n",
        "The problem is really \"a single classification problem with 57 classes\" and as such defined as \"optimizing the log loss (cross entropy)\", here \"using the ADAM optimization algorithm\" for speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM2OpAMZBrcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npFbwU5uBrcO",
        "colab_type": "text"
      },
      "source": [
        "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
        "\n",
        "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in the generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
        "\n",
        "The network is slow to train, because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in the loss is observed at the end of the epoch. We will use the best sort of weights (lowest loss), to instantiate our generative model in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmEVzveTBrcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1,\n",
        "                            save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFEKpVbFBrcW",
        "colab_type": "text"
      },
      "source": [
        "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKyFzdHWBrcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62275e71-86e8-40af-923b-5c524bf0822a"
      },
      "source": [
        "model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.9759\n",
            "Epoch 00001: loss improved from inf to 2.97587, saving model to weights-improvement-01-2.9759.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.9759\n",
            "Epoch 2/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.7970\n",
            "Epoch 00002: loss improved from 2.97587 to 2.79695, saving model to weights-improvement-02-2.7970.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.7970\n",
            "Epoch 3/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.7142\n",
            "Epoch 00003: loss improved from 2.79695 to 2.71407, saving model to weights-improvement-03-2.7141.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.7141\n",
            "Epoch 4/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.6466\n",
            "Epoch 00004: loss improved from 2.71407 to 2.64663, saving model to weights-improvement-04-2.6466.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.6466\n",
            "Epoch 5/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.5892\n",
            "Epoch 00005: loss improved from 2.64663 to 2.58923, saving model to weights-improvement-05-2.5892.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.5892\n",
            "Epoch 6/20\n",
            "1275/1279 [============================>.] - ETA: 0s - loss: 2.5368\n",
            "Epoch 00006: loss improved from 2.58923 to 2.53683, saving model to weights-improvement-06-2.5368.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.5368\n",
            "Epoch 7/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.4841\n",
            "Epoch 00007: loss improved from 2.53683 to 2.48403, saving model to weights-improvement-07-2.4840.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.4840\n",
            "Epoch 8/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.4372\n",
            "Epoch 00008: loss improved from 2.48403 to 2.43724, saving model to weights-improvement-08-2.4372.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.4372\n",
            "Epoch 9/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.3927\n",
            "Epoch 00009: loss improved from 2.43724 to 2.39278, saving model to weights-improvement-09-2.3928.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.3928\n",
            "Epoch 10/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.3509\n",
            "Epoch 00010: loss improved from 2.39278 to 2.35117, saving model to weights-improvement-10-2.3512.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.3512\n",
            "Epoch 11/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.3126\n",
            "Epoch 00011: loss improved from 2.35117 to 2.31276, saving model to weights-improvement-11-2.3128.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.3128\n",
            "Epoch 12/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.2762\n",
            "Epoch 00012: loss improved from 2.31276 to 2.27605, saving model to weights-improvement-12-2.2761.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.2761\n",
            "Epoch 13/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.2393\n",
            "Epoch 00013: loss improved from 2.27605 to 2.23934, saving model to weights-improvement-13-2.2393.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.2393\n",
            "Epoch 14/20\n",
            "1275/1279 [============================>.] - ETA: 0s - loss: 2.2084\n",
            "Epoch 00014: loss improved from 2.23934 to 2.20824, saving model to weights-improvement-14-2.2082.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.2082\n",
            "Epoch 15/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.1760\n",
            "Epoch 00015: loss improved from 2.20824 to 2.17623, saving model to weights-improvement-15-2.1762.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.1762\n",
            "Epoch 16/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.1497\n",
            "Epoch 00016: loss improved from 2.17623 to 2.14968, saving model to weights-improvement-16-2.1497.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.1497\n",
            "Epoch 17/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.1203\n",
            "Epoch 00017: loss improved from 2.14968 to 2.12046, saving model to weights-improvement-17-2.1205.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.1205\n",
            "Epoch 18/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.0957\n",
            "Epoch 00018: loss improved from 2.12046 to 2.09562, saving model to weights-improvement-18-2.0956.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.0956\n",
            "Epoch 19/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.0722\n",
            "Epoch 00019: loss improved from 2.09562 to 2.07221, saving model to weights-improvement-19-2.0722.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.0722\n",
            "Epoch 20/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.0474\n",
            "Epoch 00020: loss improved from 2.07221 to 2.04755, saving model to weights-improvement-20-2.0475.hdf5\n",
            "1279/1279 [==============================] - 16s 12ms/step - loss: 2.0475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f327c3ee2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWh99Se2Brce",
        "colab_type": "text"
      },
      "source": [
        "After running the network, we will have a number of checkpoint files in the working directory, we can delete all of them except for the one with the minimum loss.\n",
        "\n",
        "The network loss decreased almost every epoch and it can thereby be expected that the network would benefit from training for many more epochs.\n",
        "\n",
        "In the next section, let's concentrate about how we can use this model to generate new text sequeces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMhOfVggBrcg",
        "colab_type": "text"
      },
      "source": [
        "# Generating Text with an LSTM Network\n",
        "\n",
        "Generating text with this network is actually pretty straightforward now. \n",
        "Firstly, we load the data and define teh network in exactly the same way, except that:\n",
        "- the network weights are loaded from a checkpoint file, and, \n",
        "- the network does not need to trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4YpYbqZBrch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = 'weights-improvement-20-2.0475.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awz-aVOfBrco",
        "colab_type": "text"
      },
      "source": [
        "Also, when preparing the mapping of the unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfifNCqIBrcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLP2-E1Brcx",
        "colab_type": "text"
      },
      "source": [
        "Finally, we need to actually make predictions.\n",
        "\n",
        "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character and then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (eg: A sequence of 1000 characters in length).\n",
        "\n",
        "We can pick a random input pattern as our sequence, then print the generated chaaracters as we generate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S37QbLhBrcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "6a8594c5-85d8-4d1a-9554-d6d26148e977"
      },
      "source": [
        "# pick a random seed\n",
        "import sys\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "\n",
        "print('Seed: ')\n",
        "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose = 0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1: len(pattern)]\n",
        "print('\\nDone!')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            "\" she went round the court and got behind him, and\n",
            "very soon found an opportunity of taking it away. s \"\n",
            "he was not in the tooee to tee thet sas the was sored ho the caree in the rabbet sore, and she whit hn was toe tirte to tee the was soenk at inrs an the was sotnd the tas of the couro, and tae toine to be a sore of the toeet an inss to the toeee tf the career, and see woine to betit all the war sot what it was toenking to the toeee tf the caree in the caree in the rabbet sore, and she whit hn was toe tirte to tee the was sor that she was sot thsh the tone, \n",
            "'the mase thing so be in toe to tee the gocster saseer ' said the mock turtle. \n",
            "'i dane tae thing toe to toen toe toieg to tee thet ' said the mock turtle.\n",
            "\n",
            "'i dane the mabt thrn saeee'' said the maccit. \n",
            "anice was tor tas toe tirtee to tee thet sas thth the mooe, and the was tointing an the woond to tee the was sor that she was sot thsh the tone, \n",
            "'the mase thing so be in toe to tee the gocster saseer ' said the mock turtle. \n",
            "'i dane tae thing toe to toen toe toieg to tee thet ' said the mock turtle.\n",
            "\n",
            "'i dane the mabt thrn saeee'' \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-VmXTifBrc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p9epo87BrdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLytOxmgBrdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpBwDA45BrdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxRiqDsrBrdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqsRIiXyBrde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAy8QkdBBrdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JClUxt7UBrdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn9TJ0tLBrd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuJW6Ka6Brd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVTiXpNRBreA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EikWHbxxBreG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD3RcUXUBreM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iONC2D8uBreT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBveEtPWBreY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ2FRuErBref",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}